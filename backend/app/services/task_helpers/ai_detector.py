#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ai_detector.py
~~~~~~~~~~~~~~
Lightweight wrapper around the open-source model
`roberta-base-openai-detector` to estimate the probability that a text
snippet was machine-generated.

Adapted for the thesis helper system.
"""

from __future__ import annotations

import logging
from typing import Optional, Tuple
import os

try:
    import torch
    from transformers import AutoModelForSequenceClassification, AutoTokenizer
    DEPENDENCIES_AVAILABLE = True
except ImportError:
    DEPENDENCIES_AVAILABLE = False
    torch = None
    AutoModelForSequenceClassification = None
    AutoTokenizer = None

logger = logging.getLogger(__name__)

class AIDetector:
    """AI content detection service for thesis helper system."""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.model_name = "roberta-base-openai-detector"
        self._initialized = False
        
        if DEPENDENCIES_AVAILABLE:
            self._initialize_model()
    
    def _initialize_model(self):
        """Initialize the AI detection model."""
        try:
            logger.info(f"Loading AI detection model: {self.model_name}")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)
            self._initialized = True
            logger.info("AI detection model loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load AI detection model: {e}")
            self._initialized = False
    
    def detect_ai_content(self, text: str) -> Tuple[float, str]:
        """
        Return the probability (0â€“1) that *text* was generated by an LLM.
        
        Args:
            text: The text to analyze
            
        Returns:
            Tuple of (probability, explanation)
        """
        if not DEPENDENCIES_AVAILABLE:
            return 0.0, "AI detection dependencies not available"
            
        if not self._initialized:
            return 0.0, "AI detection model not initialized"
            
        if not text or not text.strip():
            return 0.0, "Empty text provided"
        
        try:
            inputs = self.tokenizer(
                text, 
                return_tensors="pt",
                truncation=True, 
                max_length=512
            )
            
            with torch.no_grad():
                logits = self.model(**inputs).logits
                probability = torch.softmax(logits, dim=1)[0, 1].item()
            
            # Generate explanation based on probability
            if probability > 0.8:
                explanation = "Very likely AI-generated content"
            elif probability > 0.6:
                explanation = "Likely AI-generated content"
            elif probability > 0.4:
                explanation = "Possibly AI-generated content"
            else:
                explanation = "Likely human-written content"
                
            return probability, explanation
            
        except Exception as e:
            logger.error(f"Error during AI detection: {e}")
            return 0.0, f"Error during analysis: {str(e)}"

# Global instance
_detector = AIDetector()

def detect_ai_content(text: str) -> Tuple[float, str]:
    """
    Detect if text is AI-generated.
    
    Args:
        text: The text to analyze
        
    Returns:
        Tuple of (probability, explanation)
    """
    return _detector.detect_ai_content(text) 